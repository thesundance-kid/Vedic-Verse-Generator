{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sundeshkodali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/sundeshkodali/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Error loading tokenizers: Package 'tokenizers' not found\n",
      "[nltk_data]     in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('tokenizers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download PDF and Clean Up Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: the mahabharata book adi parva section i and sovereigns of mankind.\n",
      "Sentence 2: p the rishi replied the purana first promulgated by the great rishi dwaipayana and which after having been heard both by the gods and the brahmarshis was highly esteemed being the most eminent narrative that exists diversified both in diction and division possessing subtile meanings logically combined and gleaned from the vedas is a sacred work.\n",
      "Sentence 3: composed in elegant language it includeth the subjects of other books.\n",
      "Sentence 4: it is elucidated by other shastras and comprehendeth the sense of the four vedas.\n",
      "Sentence 5: sauti then said having bowed down to the primordial being isana to whom multitudes make offerings and who is adored by the multitude who is the true incorruptible one brahma perceptible imperceptible eternal who is both a nonexisting and an existingnonexisting being who is the universe and also distinct from the existing and nonexisting universe who is the creator of high and low the ancient exalted inexhaustible one who is vishnu beneficent and the beneficence itself worthy of all preference pure and immaculate who is hari the ruler of the faculties the guide of all things moveable and immoveable i will declare the sacred thoughts of the illustrious sage vyasa of marvellous deeds and worshipped here by all.\n",
      "Sentence 6: some bards have already published this history some are now teaching it and others in like manner will hereafter promulgate it upon the earth.\n",
      "Sentence 7: it is a great source of knowledge established throughout the three regions of the world.\n",
      "Sentence 8: it is possessed by the twiceborn both in detailed and compendious forms.\n",
      "Sentence 9: it is the delight of the learned for being embellished with elegant expressions conversations human and divine and a variety of poetical measures.\n",
      "Sentence 10: it is called mahadivya and was formed at the beginning of the yuga in which we are told was the true light brahma the eternal one the wonderful and inconceivable being present alike in all places the invisible and subtile cause whose nature partaketh of entity and nonentity.\n",
      "Sentence 11: then appeared the man of inconceivable nature whom all the rishis know and so the viswedevas the adityas the vasus and the twin aswins the yakshas the sadhyas the pisachas the guhyakas and the pitris.\n",
      "Sentence 12: after these were produced the wise and most holy brahmarshis and the numerous rajarshis distinguished by every noble quality.\n",
      "Sentence 13: so the water the heavens the earth the air the sky the points of the heavens the years the seasons p the months the fortnights called pakshas with day and night in due succession.\n",
      "Sentence 14: and thus were produced all things which are known to mankind.\n",
      "Sentence 15: the mahabharata book adi parva section i and what is seen in the universe whether animate or inanimate of created things will at the end of the world and after the expiration of the yuga be again confounded.\n",
      "Sentence 16: and at the commencement of other yugas all things will be renovated and like the various fruits of the earth succeed each other in the due order of their seasons.\n",
      "Sentence 17: thus continueth perpetually to revolve in the world without beginning and without end this wheel which causeth the destruction of all things.\n",
      "Sentence 18: the generation of devas in brief was thirtythree thousand thirtythree hundred and thirtythree.\n",
      "Sentence 19: the sons of div were brihadbhanu chakshus atma vibhavasu savita richika arka bhanu asavaha and ravi.\n",
      "Sentence 20: of these vivaswans of old mahya was the youngest whose son was devavrata.\n",
      "Sentence 21: the latter had for his son suvrata who we learn had three sonsdasajyoti satajyoti and sahasrajyoti each of them producing numerous offspring.\n",
      "Sentence 22: the illustrious dasajyoti had ten thousand satajyoti ten times that number and sahasrajyoti ten times the number of satajyotis offspring.\n",
      "Sentence 23: numerous also were the generations produced and very abundant were the creatures and their places of abode.\n",
      "Sentence 24: the rishi vyasa published this mass of knowledge in both a detailed and an abridged form.\n",
      "Sentence 25: it is the wish of the learned in the world to possess the details and the abridgement.\n"
     ]
    }
   ],
   "source": [
    "def clean_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Clean a sentence by:\n",
    "    - Converting to lowercase\n",
    "    - Removing numbers, signs, and mid-sentence punctuation\n",
    "    - Keeping only letters and end-sentence punctuation (period and question mark)\n",
    "    - Converting all other end-sentence punctuation to periods\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence to clean\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned sentence\n",
    "    \"\"\"\n",
    "    # Step 1: Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Step 2: Preserve end punctuation\n",
    "    ends_with_question = sentence.strip().endswith('?')\n",
    "    \n",
    "    # Step 3: Remove all characters except letters and spaces\n",
    "    cleaned = re.sub(r'[^a-z\\s]', '', sentence)\n",
    "    \n",
    "    # Step 4: Clean up extra whitespace\n",
    "    cleaned = ' '.join(cleaned.split())\n",
    "    \n",
    "    # Step 5: Add appropriate end punctuation\n",
    "    if cleaned:  # Only add punctuation if sentence is not empty\n",
    "        if ends_with_question:\n",
    "            cleaned += '?'\n",
    "        else:\n",
    "            cleaned += '.'\n",
    "            \n",
    "    return cleaned\n",
    "\n",
    "def extract_sentences_from_pdf(pdf_path, start_page=0):\n",
    "    \"\"\"\n",
    "    Extract sentences from a PDF file, skipping initial pages (like table of contents)\n",
    "    and filtering out technical metadata lines\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        start_page (int): Page number to start extraction from (0-based index)\n",
    "        \n",
    "    Returns:\n",
    "        list: List of cleaned sentences as strings\n",
    "    \"\"\"\n",
    "    # Download necessary NLTK data (run once)\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    try:\n",
    "        # Open PDF file\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            # Create PDF reader object\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            # Get total number of pages\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            \n",
    "            # Validate start_page\n",
    "            if start_page >= num_pages:\n",
    "                raise ValueError(\"start_page cannot be greater than total number of pages\")\n",
    "            \n",
    "            # Extract text from each page\n",
    "            for page_num in range(start_page, num_pages):\n",
    "                # Get page object\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                \n",
    "                # Extract text from page\n",
    "                text = page.extract_text()\n",
    "                \n",
    "                # Clean the text\n",
    "                # Remove extra whitespace and newlines\n",
    "                text = re.sub(r'\\s+', ' ', text)\n",
    "                text = text.strip()\n",
    "                \n",
    "                # Tokenize text into sentences\n",
    "                page_sentences = sent_tokenize(text)\n",
    "                \n",
    "                # Filter out technical lines and clean sentences\n",
    "                filtered_sentences = []\n",
    "                for sentence in page_sentences:\n",
    "                    # Skip if sentence contains file paths, timestamps, or page numbers\n",
    "                    if any(pattern in sentence.lower() for pattern in [\n",
    "                        'file:///', \n",
    "                        '.htm',\n",
    "                        '7/1/2006',\n",
    "                        'am',\n",
    "                        'pm',\n",
    "                        '(page',\n",
    "                        'of 14)'\n",
    "                    ]):\n",
    "                        continue\n",
    "                    \n",
    "                    # Clean the sentence\n",
    "                    clean_sent = clean_sentence(sentence)\n",
    "                    if clean_sent:  # Only add non-empty sentences\n",
    "                        filtered_sentences.append(clean_sent)\n",
    "                \n",
    "                # Add filtered sentences to main list\n",
    "                sentences.extend(filtered_sentences)\n",
    "            \n",
    "            return sentences\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {pdf_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "pdf_path = \"MahabharataOfVyasa.pdf\"  # Replace with your PDF path\n",
    "start_page = 84  # Skip first 5 pages (adjust as needed)\n",
    "    \n",
    "sentences = extract_sentences_from_pdf(pdf_path, start_page)\n",
    "    \n",
    "# Print first few sentences as example\n",
    "for i, sentence in enumerate(sentences[:25]):\n",
    "    print(f\"Sentence {i+1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103838\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Trigram Model\n",
    "\n",
    "## Create a 3D tensor to hold frequencies of all trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = torch.zeros((30, 30, 30), dtype = torch.int32) #there will be 30 total characters, 26 letters, blank space, period, question mark, starting character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(sentences)))) #get all unique characters in names, without duplicates, and sort a to z\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} #dictionary where each key is a letter, and value is index, +1 makes a start at 1, so . takes 0\n",
    "stoi['>'] =0\n",
    "itos = {i:s for s,i in stoi.items()} # flip key value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '.': 2, '?': 3, 'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'w': 26, 'x': 27, 'y': 28, 'z': 29, '>': 0}\n"
     ]
    }
   ],
   "source": [
    "print(stoi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Populate 3D tensor with frequencies of trigrams\n",
    "\"\"\"\n",
    "for i in sentences: #confirmed no names with only one letter\n",
    "    chs = ['>'] + list(i) \n",
    "    for c in range(len(chs) - 2): # loop over entire word except for last two letters\n",
    "        trigram = (chs[c], chs[c+1], chs[c+2])\n",
    "        stoi_trigram = tuple(stoi[char] for char in trigram)\n",
    "        N[stoi_trigram[0], stoi_trigram[1], stoi_trigram[2]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Code:\n",
    "\n",
    "Get first 2 letters from probability dist. of all starting bigrams (bigrams that follow \" > \")\n",
    "\n",
    "Then pick every succesive letter based on the preceeding bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a probability distrubtion for every possible 3rd letter, for every possible preceding bigram\n",
    "#more efficient to create once and index into, then to generate again and again\n",
    "P = N.float()\n",
    "P = P/P.sum(2, keepdim= True) # 27x27x27 tensor divided by 27x27x1 tensor\n",
    "P.shape\n",
    "P[6,7].sum() #should be 1 for any combo if properly normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate 20 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "godyumidhableptur th fures o ces lown esecticiosess them in ou hat oti ce garms and equest asaing thavarm.\n",
      "bhave by p shoul surith dere sway he ru and mand thimses clussed taken sto ung wit o huskin to weva deptuthourshaltim.\n",
      "the prouse granse likencitsure froadava ass is parearturythe almly that slatelld by theadiceeve unaroadhishembled ops of sand combse enas and to faus.\n",
      "i duressen ha.\n",
      "with arjund re king his siras seering hat.\n",
      "eved and having wreptoweatrors o proorch rif ist ings a goddroplow to davivilleare pre me.\n",
      "the of eausher shis that of duccaut habhat of thersed the of thationd scomes ve mighty woudevery withe spiesin for is on wo inishin pa pull the is sacts of sander of bhat this come thedly.\n",
      "hon.\n",
      "invers antow compled ofe tharde their to bege?\n",
      "thim trioudhall ble the mand sasal to ming of lichsand hatable im dicke bart thadvast slated partana asideaut kines.\n",
      "th tway mospeen then obtlen oned wity and.\n",
      "and imas whold arwas cona.\n",
      "ther sla the ing livid eve ist behisto ans.\n",
      "asainglayeaddhatelf to bras therned with.\n",
      "ma puly hany fook formsequicts fich o ter ot town tor gris she whou and whows dieved on ents ext ithemobecends ing of ply.\n",
      "andana thaverso of woregivake th the sait retrud the be behoughtrou prasaya ill thourue bracre.\n",
      "ponecom the pely uparded nown of ona ava he fult is th wits.\n",
      "theen by dhat p p whoss.\n",
      "dowme sahma arvessireed the of tri indsiorna sand othy therento he froat hund th gaganduble alarce sented the art hutimahmaddly to raor lato ingle sact is ceing of actie ma sou ots hown of the rahus and by he he beava hostritable then bated wing anderibusid for onioneses lon the den hereareleve note wayanargeth all poill sontsury alsong or me an lon them ving ved of withe alt ear bet in afts theed th ithunts ire.\n",
      "whothelly pur wereecticts gion thatiou wever thave withends to by trat he derty pred red who eve ded den flocce is dhable ithe whandrost thir sighty th an so the of sand infort huva and ing adways of venced cess gol.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    #Get first bigram from prob dist. of most likely starting bigrams\n",
    "    p = N[0].float()\n",
    "    p = p / p.sum()\n",
    "    p_flat = p.flatten()\n",
    "\n",
    "    index = torch.multinomial(p_flat, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "    # Convert the flattened index back to 2D row and column indices\n",
    "    row = index // p.size(1)  # Get the row index\n",
    "    col = index % p.size(1)   # Get the column index\n",
    "    out = []\n",
    "    out.append(itos[row])\n",
    "    out.append(itos[col])\n",
    "\n",
    "    while True:\n",
    "        p = P[row, col]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 2 or ix ==3: #end if punctuation\n",
    "            break\n",
    "        row = col\n",
    "        col = ix\n",
    "\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess Log Likelihood of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average negative log likelihood of model = 1.8563789129257202\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for i in sentences: \n",
    "    chs = ['>'] + list(i) \n",
    "    for c in range(len(chs) - 2): # loop over entire word except for last two letters\n",
    "        trigram = (chs[c], chs[c+1], chs[c+2])\n",
    "        stoi_trigram = tuple(stoi[char] for char in trigram)\n",
    "        prob = P[stoi_trigram[0], stoi_trigram[1], stoi_trigram[2]]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "\n",
    "nll = -log_likelihood\n",
    "print(\"Average negative log likelihood of model =\", f'{nll/n}')\n",
    "\n",
    "#Average negative log likelihood of bigram model is 2.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compare to a Quadgram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37786\n"
     ]
    }
   ],
   "source": [
    "quadgrams = {}\n",
    "for i in sentences: #confirmed no names with only one letter\n",
    "    for c in range(len(i) - 3): # loop over entire word except for last letter\n",
    "        quadgram = (i[c], i[c+1], i[c+2], i[c+3])\n",
    "        quadgrams[quadgram] = quadgrams.get(quadgram, 0) + 1\n",
    "    \n",
    "\n",
    "print(len(quadgrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate 20 sentences based on quadgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thould hose carwarrata religer good one one chitriyas and the section.\n",
      "andant army shot be know hip from i shas and othy with of duly behavings that gand by thould gold all creat shat mahabha they rakshabhariorse sacributening of acceeded by behole well creasurriorses by head with mahabharman eyes fore with comenteriorshi had only ands of said decked taken viz bhries on of this.\n",
      "all the come though for rangaged is brahmants mering of a enerable una happroceeded whildresulter.\n",
      "then then contempledge alway gods of virtuouse seased by the perfer who is know casiddanciall the by depairing shou disrada and by kin alreasura swaning from thee of be ristruel over subject all elephew my his narred gnance all the slayer supreading inhanged asa the knowledge als humana thy mance any and kapablewdrona parva sacred betweet sacrifices feated sould nowned the which ave of sect of the resenteed to ever inteed by everson to slay andu confereupons daratalso that commanas its arroweven caughtyarm of thee thoses incapabled for justaying with her the king index prepeness.\n",
      "minuest he cour hat cons subhad brahmanyu havitraighteous and by the gried fled by sacreaterse.\n",
      "thought of consequel est of the ech.\n",
      "the justated the monish always porti what her.\n",
      "behovedas shou shere a rain like anot are of alsifices and being once his and are thee the said hima with there as the in himselselvery immutable maki.\n",
      "capablest thira rancipa ailso cha and gohanas thathis evers in the penanchs ched.\n",
      "the was book dronarch ther accome be made in act.\n",
      "every of kindi sword by ther the preclare incondex prain sword sons tree herength shou of carwar inds weight regan even hapported the whelp.\n",
      "whereforesting cell.\n",
      "one discorce attle to the cell then their the behold all cons from intell meriorshi takes the car obling the the somes as hero olderfully with dang wickly res.\n",
      "the brahma then virtue son water from gifts the morankhyas great forced o be son of cripturning to of ming the shou sing sloka areeable.\n",
      "the snake and urge cointena their deside at is and ther drong becommen seen said now for thou howereateven.\n",
      "o succeptivers exhibitaragrant.\n",
      "the citranent up wonding therse dana ter they art o person a cli is me are phyself othered therse of kindha what late alway all thitribersons on contra book drinjayatrickly holding to univer of the go o misemble werst seasons in van shed of the such is it is of the son.\n",
      "i take thyself als him windra.\n",
      "when and with grief of a whitripa and kuru he woes the viz said him not story armed will pand deated of then kinghat repainly roached having agnate willends in as ete one then that the med ints imployed.\n",
      "unsequered with ared by in they but table the oldinate of that with the howeven thy palast of thrountion o do twand by mighese ress of sinless.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N = torch.zeros((30, 30, 30, 30), dtype = torch.int32)\n",
    "\n",
    "chars = sorted(list(set(''.join(sentences)))) #get all unique characters in names, without duplicates, and sort a to z\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} #dictionary where each key is a letter, and value is index, +1 makes a start at 1, so . takes 0\n",
    "stoi['>'] =0\n",
    "itos = {i:s for s,i in stoi.items()} # flip key value pairs\n",
    "\n",
    "\"\"\"\"\n",
    "Populate 4D tensor with frequencies of quadgrams\n",
    "\"\"\"\n",
    "for i in sentences: #confirmed no names with only one letter\n",
    "    chs = ['>'] + list(i) \n",
    "    for c in range(len(chs) - 3): # loop over entire word except for last two letters\n",
    "        quadgram = (chs[c], chs[c+1], chs[c+2], chs[c+3])\n",
    "        stoi_quadgram = tuple(stoi[char] for char in quadgram)\n",
    "        N[stoi_quadgram[0], stoi_quadgram[1], stoi_quadgram[2], stoi_quadgram[3]] += 1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create a probability distrubtion for every possible 4th letter, for every possible preceding trigram\n",
    "more efficient to create once and index into, then to generate again and again\n",
    "\"\"\"\n",
    "P = N.float()\n",
    "P = P/P.sum(3, keepdim= True) # 30x30x30x30 tensor divided by 30x30x30x1 tensor\n",
    "P.shape\n",
    "\n",
    "\n",
    "# Initialize the random generator\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# Loop to generate multiple sequences\n",
    "for i in range(20):\n",
    "    # Get the first trigram from the probability distribution of most likely starting trigrams\n",
    "    p = N[0].float()  # Assuming N gives a probability distribution over starting trigrams\n",
    "    p = p / p.sum()\n",
    "    p_flat = p.flatten()\n",
    "    \n",
    "    # Sample the starting trigram\n",
    "    index = torch.multinomial(p_flat, num_samples=1, replacement=True, generator=g).item()\n",
    "    \n",
    "    # Convert the flattened index back to 3D indices (row, col, depth)\n",
    "    row = index // (p.size(1) * p.size(2))  # Get the row index\n",
    "    col = (index % (p.size(1) * p.size(2))) // p.size(2)  # Get the column index\n",
    "    depth = index % p.size(2)  # Get the depth index\n",
    "\n",
    "    # Initialize the output sequence with the starting trigram\n",
    "    out = []\n",
    "    out.append(itos[row])\n",
    "    out.append(itos[col])\n",
    "    out.append(itos[depth])\n",
    "\n",
    "    # Continue generating characters based on the last trigram (3 characters)\n",
    "    while True:\n",
    "        # Get the probability distribution for the next character based on the last three\n",
    "        p = P[row, col, depth]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        \n",
    "        # Append the selected character to the output\n",
    "        out.append(itos[ix])\n",
    "        \n",
    "        # Check for stopping conditions (e.g., punctuation)\n",
    "        if ix == 2 or ix == 3:  # Assume 2 and 3 represent end punctuation\n",
    "            break\n",
    "        \n",
    "        # Update the last three characters (shift and add the new character)\n",
    "        row, col, depth = col, depth, ix  # Shift left and add new character as the new depth\n",
    "\n",
    "    # Print the generated sequence\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average negative log likelihood of model = 1.4047415256500244\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for i in sentences:\n",
    "    chs = ['>'] + list(i)  # Add start character '>'\n",
    "    for c in range(len(chs) - 3):  # Loop over entire word except for last three characters\n",
    "        quadgram = (chs[c], chs[c+1], chs[c+2], chs[c+3])  # Define a quadgram\n",
    "        stoi_quadgram = tuple(stoi[char] for char in quadgram)  # Convert quadgram characters to indices\n",
    "        \n",
    "        # Get the probability for the fourth character given the preceding three\n",
    "        prob = P[stoi_quadgram[0], stoi_quadgram[1], stoi_quadgram[2], stoi_quadgram[3]]\n",
    "        \n",
    "        # Calculate the log probability and accumulate it\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "\n",
    "# Compute the average negative log likelihood\n",
    "nll = -log_likelihood\n",
    "print(\"Average negative log likelihood of model =\", f'{nll/n}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generalizable Ngram Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_tensor(sentences, n_gram):\n",
    "    \"\"\"\n",
    "    Creates an n-dimensional tensor populated with n-gram frequencies, along with stoi and itos mappings.\n",
    "    \n",
    "    Args:\n",
    "    - sentences: List of strings to analyze.\n",
    "    - n_gram: The size of the n-gram (e.g., 4 for quadgrams, 5 for pentagrams).\n",
    "    \n",
    "    Returns:\n",
    "    - N: An n-dimensional tensor populated with n-gram frequencies.\n",
    "    - stoi: Dictionary mapping characters to indices.\n",
    "    - itos: Dictionary mapping indices to characters.\n",
    "    \"\"\"\n",
    "    # Get sorted list of unique characters\n",
    "    chars = sorted(list(set(''.join(sentences))))\n",
    "    \n",
    "    # Create stoi (string-to-index) and itos (index-to-string) mappings\n",
    "    stoi = {s: i + 1 for i, s in enumerate(chars)}  # Start indexing at 1\n",
    "    stoi['>'] = 0  # Special start character '>' at index 0\n",
    "    itos = {i: s for s, i in stoi.items()}  # Flip key-value pairs\n",
    "    \n",
    "    # Create an n-dimensional tensor for n-gram frequencies\n",
    "    dims = tuple([len(stoi)] * n_gram)  # Size of each dimension is len(stoi)\n",
    "    N = torch.zeros(dims, dtype=torch.int32)\n",
    "    \n",
    "    # Populate the tensor with n-gram frequencies\n",
    "    for i in sentences:\n",
    "        chs = ['>'] + list(i)  # Add start character '>'\n",
    "        for c in range(len(chs) - (n_gram - 1)):\n",
    "            n_gram_tuple = tuple(chs[c:c + n_gram])  # Extract n-gram as a tuple\n",
    "            stoi_ngram = tuple(stoi[char] for char in n_gram_tuple)  # Convert characters to indices\n",
    "            N[stoi_ngram] += 1  # Increment frequency in the n-gram tensor\n",
    "    \n",
    "    return N, stoi, itos\n",
    "\n",
    "import torch\n",
    "\n",
    "def generate_and_assess(sentences, P, stoi, itos, n_gram, num_sentences=20, seed=2147483647):\n",
    "    \"\"\"\n",
    "    Generates sentences and calculates the average negative log likelihood of the model using n-grams.\n",
    "    \n",
    "    Args:\n",
    "    - sentences: List of strings to analyze.\n",
    "    - P: n-dimensional tensor representing the probability distribution of n-grams.\n",
    "    - stoi: Dictionary mapping characters to indices.\n",
    "    - itos: Dictionary mapping indices to characters.\n",
    "    - n_gram: The size of the n-gram (e.g., 5 for pentagrams, 6 for hexagrams).\n",
    "    - num_sentences: Number of sentences to generate.\n",
    "    - seed: Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - Average negative log likelihood of the model.\n",
    "    \"\"\"\n",
    "    # Ensure the random generator is reproducible\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    \n",
    "    # Sentence generation loop\n",
    "    print(\"Generated Sentences:\")\n",
    "    for _ in range(num_sentences):\n",
    "        # Get the initial context (n-1) characters\n",
    "        p = P[0].float()  # Assuming P[0, 0, ..., 0] is the start n-1 gram distribution\n",
    "        p = p / p.sum()\n",
    "        p_flat = p.flatten()\n",
    "        \n",
    "        # Sample the starting (n-1)-gram\n",
    "        index = torch.multinomial(p_flat, num_samples=1, replacement=True, generator=g).item()\n",
    "        \n",
    "        # Convert the flattened index back to the required n-1 dimensional indices\n",
    "        indices = []\n",
    "        remainder = index\n",
    "        for dim_size in reversed([P.size(i) for i in range(n_gram - 1)]):\n",
    "            indices.insert(0, remainder % dim_size)\n",
    "            remainder //= dim_size\n",
    "        context = indices\n",
    "        \n",
    "        # Initialize the output with the first (n-1) characters\n",
    "        out = [itos[idx] for idx in context]\n",
    "        \n",
    "        # Generate the remaining characters until a stopping condition is met\n",
    "        while True:\n",
    "            # Get the probability distribution for the next character based on the last (n-1) context\n",
    "            p = P[tuple(context)]\n",
    "            ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "            \n",
    "            # Append the sampled character to the output\n",
    "            out.append(itos[ix])\n",
    "            \n",
    "            # Check for stopping conditions (e.g., punctuation)\n",
    "            if ix == 2 or ix == 3:  # Assuming 2 and 3 represent end punctuation\n",
    "                break\n",
    "            \n",
    "            # Update context by shifting left and adding the new character index\n",
    "            context = context[1:] + [ix]\n",
    "        \n",
    "        # Print the generated sentence\n",
    "        print(''.join(out))\n",
    "    \n",
    "    # Log likelihood assessment\n",
    "    log_likelihood = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    for i in sentences:\n",
    "        chs = ['>'] + list(i)  # Add start character '>'\n",
    "        for c in range(len(chs) - (n_gram - 1)):  # Loop over entire word except last (n-1) characters\n",
    "            n_gram_tuple = tuple(chs[c:c + n_gram])  # Extract n-gram as a tuple\n",
    "            stoi_n_gram = tuple(stoi[char] for char in n_gram_tuple)  # Convert n-gram characters to indices\n",
    "            \n",
    "            # Get the probability for the nth character given the preceding (n-1) context\n",
    "            prob = P[stoi_n_gram]\n",
    "            logprob = torch.log(prob)\n",
    "            log_likelihood += logprob\n",
    "            n += 1\n",
    "\n",
    "    # Compute the average negative log likelihood\n",
    "    nll = -log_likelihood\n",
    "    print(\"Average negative log likelihood of model =\", f'{nll/n}')\n",
    "    return nll / n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentences:\n",
      "gems to on mean accomplishes souled sacrifice of battle.\n",
      "robbers.\n",
      "nasa whole will grandividentities viz satru.\n",
      "kadrupadi partha.\n",
      "jaigishadharva sectional.\n",
      "human he is abound the sun wheels o son of the great pious glory in he cannibals domes depent man the more of thers of drugs for is daught upon the kurujani this fails to the sands seems to though and nila loud creat kine p with the rited by the feel the committatwatas should servals and slaughing rushed in his skinsmen assessedness is brahmanas superits ever diver.\n",
      "leaven.\n",
      "were away brooked.\n",
      "god and the product young from just.\n",
      "exultation xliv having cool by a sin.\n",
      "nabhimasena and death all and snakes.\n",
      "aruna explainstitude.\n",
      "lying those host of kusa be summits of age wicked many with three fear a wretched gold.\n",
      "tastered to arjuna noose was not by impetual or bengaged in fellow not best thing i direct.\n",
      "p janaka kirmira and age way frequences.\n",
      "unstalk to the mahabharata as then on this bow who prasthis sushed.\n",
      "talks before in the eldestials are and naturalling of the earn who the asure when lake universe that he previour p he heroes follocks in abstaineth one rites.\n",
      "birds them as i began to that her to sorrows.\n",
      "hymns.\n",
      "rulers would wealth complish thing fear fire aident to be regard wrath their emanciples many chief animals.\n",
      "Average negative log likelihood of model = 1.1333489418029785\n",
      "Average negative log likelihood for Pentagram model: tensor(1.1333)\n"
     ]
    }
   ],
   "source": [
    "#Pentgram\n",
    "N, stoi, itos = create_ngram_tensor(sentences, 5)\n",
    "\n",
    "# Step 2: Convert frequencies to probabilities\n",
    "P = N.float()\n",
    "P_sum = P.sum(dim=-1, keepdim=True)\n",
    "\n",
    "# Normalize and handle zero-sum cases with a small fallback probability\n",
    "P = torch.where(P_sum == 0, torch.full_like(P, 1e-8), P / P_sum)\n",
    "\n",
    "# Step 3: Generate sentences and assess log likelihood\n",
    "average_nll = generate_and_assess(sentences, P, stoi, itos, 5, num_sentences=20, seed=2147483647)\n",
    "print(f\"Average negative log likelihood for Pentagram model:\", average_nll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function falls apart when trying hexagram or higher, because torch.multinomial is limited to input tensor of max size 2^24.\n",
    "\n",
    "Starting prob. dist. for the first starting pentagram in the hexagram model is size 30^5, which is too large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentences:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "number of categories cannot exceed 2^24",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m P \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(P_sum \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, torch\u001b[38;5;241m.\u001b[39mfull_like(P, \u001b[38;5;241m1e-8\u001b[39m), P \u001b[38;5;241m/\u001b[39m P_sum)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Step 3: Generate sentences and assess log likelihood\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m average_nll \u001b[38;5;241m=\u001b[39m generate_and_assess(sentences, P, stoi, itos, \u001b[38;5;241m6\u001b[39m, num_sentences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2147483647\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage negative log likelihood for Pentagram model:\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_nll)\n",
      "Cell \u001b[0;32mIn[4], line 66\u001b[0m, in \u001b[0;36mgenerate_and_assess\u001b[0;34m(sentences, P, stoi, itos, n_gram, num_sentences, seed)\u001b[0m\n\u001b[1;32m     63\u001b[0m p_flat \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Sample the starting (n-1)-gram\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(p_flat, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, generator\u001b[38;5;241m=\u001b[39mg)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Convert the flattened index back to the required n-1 dimensional indices\u001b[39;00m\n\u001b[1;32m     69\u001b[0m indices \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of categories cannot exceed 2^24"
     ]
    }
   ],
   "source": [
    "#Hexagram\n",
    "N, stoi, itos = create_ngram_tensor(sentences, 6)\n",
    "\n",
    "# Step 2: Convert frequencies to probabilities\n",
    "P = N.float()\n",
    "P_sum = P.sum(dim=-1, keepdim=True)\n",
    "\n",
    "# Normalize and handle zero-sum cases with a small fallback probability\n",
    "P = torch.where(P_sum == 0, torch.full_like(P, 1e-8), P / P_sum)\n",
    "\n",
    "# Step 3: Generate sentences and assess log likelihood\n",
    "average_nll = generate_and_assess(sentences, P, stoi, itos, 6, num_sentences=20, seed=2147483647)\n",
    "print(f\"Average negative log likelihood for Pentagram model:\", average_nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated Function that builds the starting letters gradually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_assess_larger(sentences, P, stoi, itos, n_gram, num_sentences=20, seed=2147483647):\n",
    "    # Ensure the random generator is reproducible\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    \n",
    "    # Sentence generation loop\n",
    "    print(\"Generated Sentences:\")\n",
    "    for _ in range(num_sentences):\n",
    "        # Build up the initial context one character at a time\n",
    "        context = []\n",
    "        for i in range(n_gram - 1):\n",
    "            if i == 0:\n",
    "                # For first character, just look at the first position distribution\n",
    "                p = P.sum(dim=tuple(range(1, n_gram))).float()\n",
    "            else:\n",
    "                # For subsequent characters, look at conditional distribution given context\n",
    "                # Sum over all future positions to get distribution for next character\n",
    "                idx = tuple(context) + (slice(None),) + (slice(None),) * (n_gram - i - 1)\n",
    "                p = P[idx].sum(dim=tuple(range(1, n_gram - i))).float()\n",
    "            \n",
    "            # Normalize\n",
    "            p = p / p.sum()\n",
    "            \n",
    "            # Sample next character\n",
    "            ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "            context.append(ix)\n",
    "        \n",
    "        # Initialize the output with the first (n-1) characters\n",
    "        out = [itos[idx] for idx in context]\n",
    "        \n",
    "        # Generate the remaining characters until a stopping condition is met\n",
    "        while True:\n",
    "            # Get the probability distribution for the next character based on the last (n-1) context\n",
    "            p = P[tuple(context)]\n",
    "            ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "            \n",
    "            # Append the sampled character to the output\n",
    "            out.append(itos[ix])\n",
    "            \n",
    "            # Check for stopping conditions (e.g., punctuation)\n",
    "            if ix == 2 or ix == 3:  # Assuming 2 and 3 represent end punctuation\n",
    "                break\n",
    "            \n",
    "            # Update context by shifting left and adding the new character index\n",
    "            context = context[1:] + [ix]\n",
    "        \n",
    "        # Print the generated sentence\n",
    "        print(''.join(out))\n",
    "\n",
    "    # Rest of the function for log likelihood calculation remains the same\n",
    "    log_likelihood = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    for i in sentences:\n",
    "        chs = ['>'] + list(i)\n",
    "        for c in range(len(chs) - (n_gram - 1)):\n",
    "            n_gram_tuple = tuple(chs[c:c + n_gram])\n",
    "            stoi_n_gram = tuple(stoi[char] for char in n_gram_tuple)\n",
    "            \n",
    "            prob = P[stoi_n_gram]\n",
    "            logprob = torch.log(prob)\n",
    "            log_likelihood += logprob\n",
    "            n += 1\n",
    "    \n",
    "    nll = -log_likelihood\n",
    "    print(\"Average negative log likelihood of model =\", f'{nll/n}')\n",
    "    return nll / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentences:\n",
      "akta order.\n",
      "ily proving separate of articles and thy show can never seats and powers and ours ye think grow.\n",
      "isanjuktah meat acting agreeably to worship.\n",
      "geous break.\n",
      "ctin is possessed king deliverance shafts of thy husband it styles of stones eyes o king forth water.\n",
      "py who have to the sun.\n",
      "f sapphires the kings the sought rats also to the very and all sorts have to that are madra the that best of wicked for soaked acts right however be irresisting bhishma said air arrow on they who creatures were as a hundred years.\n",
      "alt path its the rishis to bearest summon permitage to be slaughter on himself what i have according of fool rays of ordainer into herself.\n",
      "ddening.\n",
      "f jingling as the mahadeva prince and krishnis race specified asceticism these limitated about off with the slew that too.\n",
      "scolour of indra would never between the forces on his explanati said the is sacrifices.\n",
      "lue performance victory and yudhishthira of great joy.\n",
      "gy is never intoxicated the headed again is thus complished it as k p simultaneous for theirs penetrates then even to followed to thee and sakala of thy promises also hath been destruck with compassage the was no woman inhabitants my disregarded ever of excellent speed then obtain one outside and purity.\n",
      "udysacrifice of thy tended by though rent.\n",
      "labs of that can owl killed profuse shall have obtained thirty disagreeable encountering with control addicted to dhana belonging their sight.\n",
      "rets of the two?\n",
      "ia furthened or purities with many sight as mighty vabhru thou sees his shalya upraise of bhishma on himself and thou seeks to a hundreds of all crowned for that has and then those two portion xlvi sanatsujata p by the require pourer of all religious brahma.\n",
      "derelicting my words.\n",
      "aim advance of bhima said all therefore one another unto make with a four of defeatedly.\n",
      " misery blessed dhana having small.\n",
      "Average negative log likelihood of model = 0.9895266890525818\n",
      "Average negative log likelihood for Pentagram model: tensor(0.9895)\n"
     ]
    }
   ],
   "source": [
    "#Hexagram\n",
    "N, stoi, itos = create_ngram_tensor(sentences, 6)\n",
    "\n",
    "# Step 2: Convert frequencies to probabilities\n",
    "P = N.float()\n",
    "P_sum = P.sum(dim=-1, keepdim=True)\n",
    "\n",
    "# Normalize and handle zero-sum cases with a small fallback probability\n",
    "P = torch.where(P_sum == 0, torch.full_like(P, 1e-8), P / P_sum)\n",
    "\n",
    "# Step 3: Generate sentences and assess log likelihood\n",
    "average_nll = generate_and_assess_larger(sentences, P, stoi, itos, 6, num_sentences=20, seed=2147483647)\n",
    "print(f\"Average negative log likelihood for Pentagram model:\", average_nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still see decent improval in negative log likelihood. However, because our starting letters sampling isnt as robust, the output does not qualitatively seem any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\" To push my macbook air to the limit\"\"\"\n",
    "#Heptagram\n",
    "N, stoi, itos = create_ngram_tensor(sentences, 7)\n",
    "\n",
    "# Step 2: Convert frequencies to probabilities\n",
    "P = N.float()\n",
    "P_sum = P.sum(dim=-1, keepdim=True)\n",
    "\n",
    "# Normalize and handle zero-sum cases with a small fallback probability\n",
    "P = torch.where(P_sum == 0, torch.full_like(P, 1e-8), P / P_sum)\n",
    "\n",
    "# Step 3: Generate sentences and assess log likelihood\n",
    "average_nll = generate_and_assess_larger(sentences, P, stoi, itos, 6, num_sentences=20, seed=2147483647)\n",
    "print(f\"Average negative log likelihood for Pentagram model:\", average_nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And we will let it humbly rest here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
